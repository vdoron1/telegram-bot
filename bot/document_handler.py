import os
import json
import docx
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
#from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_huggingface import HuggingFaceEmbeddings

from langchain_community.vectorstores import FAISS

# –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º
DATA_FOLDER = "data"
VECTOR_DB_PATH = "vector_db"
USER_QUESTIONS_FILE = "user_questions.json"

# –°–æ–∑–¥–∞—ë–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (Hugging Face)
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")


# üîπ **–§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–æ–≤**
def load_files():
    docs = []

    if os.path.exists(DATA_FOLDER):
        for filename in os.listdir(DATA_FOLDER):
            file_path = os.path.join(DATA_FOLDER, filename)

            if filename.endswith(".pdf"):
                loader = PyPDFLoader(file_path)
                docs.extend(loader.load())

            elif filename.endswith(".txt"):
                with open(file_path, "r", encoding="utf-8") as file:
                    text = file.read()
                    docs.append({"page_content": text})

            elif filename.endswith(".docx"):
                doc = docx.Document(file_path)
                text = "\n".join([p.text for p in doc.paragraphs])
                docs.append({"page_content": text})

    return docs


# üîπ **–°–æ–∑–¥–∞–Ω–∏–µ –∏–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã**
def create_vector_db():
    print("üîπ –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª—ã –≤ –±–∞–∑—É...")
    documents = load_files()

    if not documents:
        print("‚ö†Ô∏è –ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
        return

    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –∫—É—Å–∫–∏
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = text_splitter.split_documents(documents)

    # –°–æ–∑–¥–∞—ë–º FAISS –±–∞–∑—É
    vector_db = FAISS.from_documents(texts, embedding=embeddings)
    vector_db.save_local(VECTOR_DB_PATH)
    print("‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∞!")


# üîπ **–§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤**
def load_user_questions():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π."""
    if os.path.exists(USER_QUESTIONS_FILE):
        with open(USER_QUESTIONS_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return []


# üîπ **–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π**
def save_user_question(question, answer):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–æ–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ –±–∞–∑—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è."""
    user_data = load_user_questions()
    user_data.append({"question": question, "answer": answer})

    with open(USER_QUESTIONS_FILE, "w", encoding="utf-8") as f:
        json.dump(user_data, f, ensure_ascii=False, indent=4)
